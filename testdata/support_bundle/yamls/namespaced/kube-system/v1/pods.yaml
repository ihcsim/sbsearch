apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 969d1f125d1734bf71fd2ecd5b8ab1b2
      kubernetes.io/config.mirror: 969d1f125d1734bf71fd2ecd5b8ab1b2
      kubernetes.io/config.seen: "2025-12-30T21:46:48.038780543Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-12-30T21:46:48Z"
    generation: 1
    labels:
      component: cloud-controller-manager
      tier: control-plane
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubernetes.io/config.hash: {}
            f:kubernetes.io/config.mirror: {}
            f:kubernetes.io/config.seen: {}
            f:kubernetes.io/config.source: {}
          f:labels:
            .: {}
            f:component: {}
            f:tier: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"75908add-436d-4a4a-ac7f-91584193f4b7"}: {}
        f:spec:
          f:containers:
            k:{"name":"cloud-controller-manager"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"FILE_HASH"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":10258,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:startupProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/ca-certificates"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/etc/ssl/certs"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/cred/cloud-controller.kubeconfig"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/etc/cloud-config.yaml"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-rke2-cloud-controller.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-rke2-cloud-controller.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/server-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeName: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"dir0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"dir1"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file1"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file2"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file3"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file4"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
      manager: kubelet
      operation: Update
      time: "2025-12-30T21:46:48Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:52:05Z"
    name: cloud-controller-manager-isim-dev
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: isim-dev
      uid: 75908add-436d-4a4a-ac7f-91584193f4b7
    resourceVersion: "8290"
    uid: 460f576c-bcb7-4e86-8ca2-93b0befcfb63
  spec:
    containers:
    - args:
      - --allocate-node-cidrs=true
      - --authentication-kubeconfig=/var/lib/rancher/rke2/server/cred/cloud-controller.kubeconfig
      - --authorization-kubeconfig=/var/lib/rancher/rke2/server/cred/cloud-controller.kubeconfig
      - --bind-address=127.0.0.1
      - --cloud-config=/var/lib/rancher/rke2/server/etc/cloud-config.yaml
      - --cloud-provider=rke2
      - --cluster-cidr=10.52.0.0/16
      - --configure-cloud-routes=false
      - --controllers=*,-route,-service
      - --kubeconfig=/var/lib/rancher/rke2/server/cred/cloud-controller.kubeconfig
      - --leader-elect-resource-name=rke2-cloud-controller-manager
      - --node-status-update-frequency=1m0s
      - --profiling=false
      command:
      - cloud-controller-manager
      env:
      - name: FILE_HASH
        value: 970c80b36731cb7e01058b4f49495c305e40aec525c398ca68489de14f55cb72
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      image: index.docker.io/rancher/rke2-cloud-provider:v1.34.2-0.20251010190833-cf0d35a732d1-build20251017
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: localhost
          path: /healthz
          port: 10258
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: cloud-controller-manager
      ports:
      - containerPort: 10258
        hostPort: 10258
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        privileged: false
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: localhost
          path: /healthz
          port: 10258
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: dir0
      - mountPath: /etc/ca-certificates
        name: dir1
      - mountPath: /var/lib/rancher/rke2/server/cred/cloud-controller.kubeconfig
        name: file0
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/etc/cloud-config.yaml
        name: file1
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-rke2-cloud-controller.crt
        name: file2
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-rke2-cloud-controller.key
        name: file3
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.crt
        name: file4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 2e+09
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: dir0
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: dir1
    - hostPath:
        path: /var/lib/rancher/rke2/server/cred/cloud-controller.kubeconfig
        type: File
      name: file0
    - hostPath:
        path: /var/lib/rancher/rke2/server/etc/cloud-config.yaml
        type: File
      name: file1
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-rke2-cloud-controller.crt
        type: File
      name: file2
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-rke2-cloud-controller.key
        type: File
      name: file3
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/server-ca.crt
        type: File
      name: file4
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 128Mi
      containerID: containerd://350dafbaf6f6dcc5b040342693a7160ab1f3abc5d4e5b383bc4f31dcd540f75f
      image: docker.io/rancher/rke2-cloud-provider:v1.34.2-0.20251010190833-cf0d35a732d1-build20251017
      imageID: sha256:35741f48740f4b7acbb33e53dead272c7187151e574c4dc0dce09d9b037764f8
      lastState: {}
      name: cloud-controller-manager
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:46:48Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    phase: Running
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: Burstable
    startTime: "2025-12-30T21:51:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      etcd.k3s.io/initial: '{"initial-advertise-peer-urls":"https://192.168.122.168:2380","initial-cluster":"isim-dev-2ed4e222=https://192.168.122.168:2380","initial-cluster-state":"new"}'
      kubernetes.io/config.hash: fa1e95c6384ed263a0133bbdcd4ccfc8
      kubernetes.io/config.mirror: fa1e95c6384ed263a0133bbdcd4ccfc8
      kubernetes.io/config.seen: "2025-12-30T21:46:28.125865447Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-12-30T21:46:43Z"
    generation: 1
    labels:
      component: etcd
      tier: control-plane
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:etcd.k3s.io/initial: {}
            f:kubernetes.io/config.hash: {}
            f:kubernetes.io/config.mirror: {}
            f:kubernetes.io/config.seen: {}
            f:kubernetes.io/config.source: {}
          f:labels:
            .: {}
            f:component: {}
            f:tier: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"75908add-436d-4a4a-ac7f-91584193f4b7"}: {}
        f:spec:
          f:containers:
            k:{"name":"etcd"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"FILE_HASH"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":2379,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":2380,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":2381,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/db/etcd"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/db/etcd/config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/etcd/peer-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/etcd/peer-server-client.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/etcd/peer-server-client.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/etcd/server-client.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/etcd/server-client.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeName: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"dir0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file1"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file2"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file3"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file4"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file5"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file6"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
      manager: kubelet
      operation: Update
      time: "2025-12-30T21:46:43Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:51:55Z"
    name: etcd-isim-dev
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: isim-dev
      uid: 75908add-436d-4a4a-ac7f-91584193f4b7
    resourceVersion: "8042"
    uid: fffd8ae4-37de-4dda-af73-e1fc444d5e34
  spec:
    containers:
    - args:
      - --config-file=/var/lib/rancher/rke2/server/db/etcd/config
      command:
      - etcd
      env:
      - name: FILE_HASH
        value: 5f78b9906150b0eb814712592306cd11cf9a487767726171a791b2427450f635
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      image: index.docker.io/rancher/hardened-etcd:v3.6.5-k3s1-build20251017
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: localhost
          path: /health?serializable=true
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      ports:
      - containerPort: 2379
        hostPort: 2379
        name: client
        protocol: TCP
      - containerPort: 2380
        hostPort: 2380
        name: peer
        protocol: TCP
      - containerPort: 2381
        hostPort: 2381
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rancher/rke2/server/db/etcd
        name: dir0
      - mountPath: /var/lib/rancher/rke2/server/tls/etcd/server-client.crt
        name: file0
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/etcd/server-client.key
        name: file1
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt
        name: file2
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.crt
        name: file3
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.key
        name: file4
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/etcd/peer-ca.crt
        name: file5
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/db/etcd/config
        name: file6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 2e+09
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/rancher/rke2/server/db/etcd
        type: DirectoryOrCreate
      name: dir0
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/etcd/server-client.crt
        type: File
      name: file0
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/etcd/server-client.key
        type: File
      name: file1
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt
        type: File
      name: file2
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.crt
        type: File
      name: file3
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.key
        type: File
      name: file4
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/etcd/peer-ca.crt
        type: File
      name: file5
    - hostPath:
        path: /var/lib/rancher/rke2/server/db/etcd/config
        type: File
      name: file6
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 200m
        memory: 512Mi
      containerID: containerd://2c260426b66d771e07191051b98010499fe371344d0e14969a54758cf5aed9e1
      image: docker.io/rancher/hardened-etcd:v3.6.5-k3s1-build20251017
      imageID: sha256:405516f27f18a8850d1f39211de4a716650e356cbde556f820580413c57625af
      lastState: {}
      name: etcd
      ready: true
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:46:28Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    phase: Running
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: Burstable
    startTime: "2025-12-30T21:51:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-30T21:49:31Z"
    generateName: harvester-whereabouts-
    generation: 1
    labels:
      app: whereabouts
      app.kubernetes.io/instance: harvester
      app.kubernetes.io/name: whereabouts
      controller-revision-hash: 558d99c66d
      name: whereabouts
      pod-template-generation: "1"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:app: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/name: {}
            f:controller-revision-hash: {}
            f:name: {}
            f:pod-template-generation: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"8b5d4e9d-6197-4b90-8f39-c7c2b3638e58"}: {}
        f:spec:
          f:affinity:
            .: {}
            f:nodeAffinity:
              .: {}
              f:requiredDuringSchedulingIgnoredDuringExecution: {}
          f:containers:
            k:{"name":"whereabouts"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"NODENAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"WHEREABOUTS_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources:
                .: {}
                f:limits:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/cron-schedule"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/host/etc/cni/net.d"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/host/opt/cni/bin"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"cni-net-dir"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"cnibin"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"cron-scheduler-configmap"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:items: {}
                f:name: {}
              f:name: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:49:31Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              f:observedGeneration: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:observedGeneration: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:38Z"
    name: harvester-whereabouts-w5wc5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: harvester-whereabouts
      uid: 8b5d4e9d-6197-4b90-8f39-c7c2b3638e58
    resourceVersion: "4366"
    uid: ff80d90b-d7c3-4569-8fe7-40148a65e7bd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - isim-dev
    containers:
    - args:
      - -c
      - |
        SLEEP=false source /install-cni.sh
        /token-watcher.sh &
        /ip-control-loop -log-level debug
      command:
      - /bin/sh
      env:
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: WHEREABOUTS_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: ghcr.io/k8snetworkplumbingwg/whereabouts:v0.9.2
      imagePullPolicy: IfNotPresent
      name: whereabouts
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cnibin
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /cron-schedule
        name: cron-scheduler-configmap
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7k8l9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: harvester-whereabouts
    serviceAccountName: harvester-whereabouts
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /opt/cni/bin
        type: "null"
      name: cnibin
    - hostPath:
        path: /etc/cni/net.d
        type: "null"
      name: cni-net-dir
    - configMap:
        defaultMode: 484
        items:
        - key: cron-expression
          path: config
        name: whereabouts-config
      name: cron-scheduler-configmap
    - name: kube-api-access-7k8l9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:37Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:31Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:37Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 100Mi
      containerID: containerd://916e9b8ef24b892c5252c341b81ec22b8bf42b054effbcfa6bf6df8de9abf35d
      image: ghcr.io/k8snetworkplumbingwg/whereabouts:v0.9.2
      imageID: sha256:fb5d855c1e441aff51cdd8c2929d8051612b1023d9bfe66ca25658efba2332cd
      lastState: {}
      name: whereabouts
      ready: true
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 100Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:49:37Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          - 1
          - 2
          - 3
          - 4
          - 6
          - 10
          - 11
          - 20
          - 26
          - 27
          uid: 0
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cnibin
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /cron-schedule
        name: cron-scheduler-configmap
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7k8l9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Running
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: Burstable
    startTime: "2025-12-30T21:49:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=1DAE88CB86DC838C659BAC15D7FAFD30F1AA665C5590527D5AC1E97634CA4851
    creationTimestamp: "2025-12-30T21:46:52Z"
    generateName: helm-install-rke2-canal-
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 87391999-5879-43d4-ba72-b1a433b8ef4b
      batch.kubernetes.io/job-name: helm-install-rke2-canal
      controller-uid: 87391999-5879-43d4-ba72-b1a433b8ef4b
      helmcharts.helm.cattle.io/chart: rke2-canal
      job-name: helm-install-rke2-canal
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:helmcharts.helm.cattle.io/configHash: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:batch.kubernetes.io/controller-uid: {}
            f:batch.kubernetes.io/job-name: {}
            f:controller-uid: {}
            f:helmcharts.helm.cattle.io/chart: {}
            f:job-name: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"87391999-5879-43d4-ba72-b1a433b8ef4b"}: {}
        f:spec:
          f:containers:
            k:{"name":"helm"}:
              .: {}
              f:args: {}
              f:env:
                .: {}
                k:{"name":"AUTH_PASS_CREDENTIALS"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"BOOTSTRAP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CHART"}:
                  .: {}
                  f:name: {}
                k:{"name":"CHART_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FAILURE_POLICY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_DRIVER"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_VERSION"}:
                  .: {}
                  f:name: {}
                k:{"name":"INSECURE_SKIP_TLS_VERIFY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"KUBERNETES_SERVICE_HOST"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"KUBERNETES_SERVICE_PORT"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NAME"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"PLAIN_HTTP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"REPO"}:
                  .: {}
                  f:name: {}
                k:{"name":"TARGET_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"VERSION"}:
                  .: {}
                  f:name: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/chart"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.cache"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.helm"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/tmp"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeSelector: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext:
            .: {}
            f:runAsNonRoot: {}
            f:seccompProfile:
              .: {}
              f:type: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"content"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"klipper-cache"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-config"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-helm"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"tmp"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"values"}:
              .: {}
              f:name: {}
              f:projected:
                .: {}
                f:defaultMode: {}
                f:sources: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:46:52Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              f:observedGeneration: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:observedGeneration: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:46:57Z"
    name: helm-install-rke2-canal-ldd4k
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-rke2-canal
      uid: 87391999-5879-43d4-ba72-b1a433b8ef4b
    resourceVersion: "777"
    uid: d4bd3dc0-87c5-47c1-b94a-a1677d7c943f
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.clusterCIDR=10.52.0.0/16
      - --set-string
      - global.clusterCIDRv4=10.52.0.0/16
      - --set-string
      - global.clusterDNS=10.53.0.10
      - --set-string
      - global.clusterDomain=cluster.local
      - --set-string
      - global.rke2DataDir=/var/lib/rancher/rke2
      - --set-string
      - global.serviceCIDR=10.53.0.0/16
      - --set-string
      - global.systemDefaultIngressClass=ingress-nginx
      env:
      - name: NAME
        value: rke2-canal
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 127.0.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "6443"
      - name: BOOTSTRAP
        value: "true"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.10-build20251111
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-phzbv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
      node-role.kubernetes.io/control-plane: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-rke2-canal
    serviceAccountName: helm-rke2-canal
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Equal
      value: "true"
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node-role.kubernetes.io/etcd
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: chart-values-rke2-canal
    - configMap:
        defaultMode: 420
        name: chart-content-rke2-canal
      name: content
    - name: kube-api-access-phzbv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:57Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:52Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:55Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:55Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:52Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://30a1b781b9abad0e338edc0c58a2b19bc5c41f56880dd0c79bedc990b2f06094
      image: docker.io/rancher/klipper-helm:v0.9.10-build20251111
      imageID: sha256:1d0854c41ff27a347388550f509052958313f7cb0b013a73bc0f5cf3978e4053
      lastState: {}
      name: helm
      ready: false
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://30a1b781b9abad0e338edc0c58a2b19bc5c41f56880dd0c79bedc990b2f06094
          exitCode: 0
          finishedAt: "2025-12-30T21:46:54Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-12-30T21:46:53Z"
      user:
        linux:
          gid: 1000
          supplementalGroups:
          - 1000
          uid: 1000
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-phzbv
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Succeeded
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: BestEffort
    startTime: "2025-12-30T21:46:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=97583F5A79066F0221291B603A203BD46AF3B19C2ED170B28292EE56E42B5C0F
    creationTimestamp: "2025-12-30T21:46:52Z"
    generateName: helm-install-rke2-coredns-
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 6b1990f1-a4db-497e-8371-4103c02da93f
      batch.kubernetes.io/job-name: helm-install-rke2-coredns
      controller-uid: 6b1990f1-a4db-497e-8371-4103c02da93f
      helmcharts.helm.cattle.io/chart: rke2-coredns
      job-name: helm-install-rke2-coredns
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:helmcharts.helm.cattle.io/configHash: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:batch.kubernetes.io/controller-uid: {}
            f:batch.kubernetes.io/job-name: {}
            f:controller-uid: {}
            f:helmcharts.helm.cattle.io/chart: {}
            f:job-name: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"6b1990f1-a4db-497e-8371-4103c02da93f"}: {}
        f:spec:
          f:containers:
            k:{"name":"helm"}:
              .: {}
              f:args: {}
              f:env:
                .: {}
                k:{"name":"AUTH_PASS_CREDENTIALS"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"BOOTSTRAP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CHART"}:
                  .: {}
                  f:name: {}
                k:{"name":"CHART_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FAILURE_POLICY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_DRIVER"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_VERSION"}:
                  .: {}
                  f:name: {}
                k:{"name":"INSECURE_SKIP_TLS_VERIFY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"KUBERNETES_SERVICE_HOST"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"KUBERNETES_SERVICE_PORT"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NAME"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"PLAIN_HTTP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"REPO"}:
                  .: {}
                  f:name: {}
                k:{"name":"TARGET_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"VERSION"}:
                  .: {}
                  f:name: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/chart"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.cache"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.helm"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/tmp"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeSelector: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext:
            .: {}
            f:runAsNonRoot: {}
            f:seccompProfile:
              .: {}
              f:type: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"content"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"klipper-cache"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-config"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-helm"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"tmp"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"values"}:
              .: {}
              f:name: {}
              f:projected:
                .: {}
                f:defaultMode: {}
                f:sources: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:46:52Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              f:observedGeneration: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:observedGeneration: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:46:57Z"
    name: helm-install-rke2-coredns-x6znp
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-rke2-coredns
      uid: 6b1990f1-a4db-497e-8371-4103c02da93f
    resourceVersion: "766"
    uid: d4315577-4aeb-4fff-af54-d3b453964b1d
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.clusterCIDR=10.52.0.0/16
      - --set-string
      - global.clusterCIDRv4=10.52.0.0/16
      - --set-string
      - global.clusterDNS=10.53.0.10
      - --set-string
      - global.clusterDomain=cluster.local
      - --set-string
      - global.rke2DataDir=/var/lib/rancher/rke2
      - --set-string
      - global.serviceCIDR=10.53.0.0/16
      - --set-string
      - global.systemDefaultIngressClass=ingress-nginx
      env:
      - name: NAME
        value: rke2-coredns
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 127.0.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "6443"
      - name: BOOTSTRAP
        value: "true"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.10-build20251111
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sl96p
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
      node-role.kubernetes.io/control-plane: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-rke2-coredns
    serviceAccountName: helm-rke2-coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Equal
      value: "true"
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node-role.kubernetes.io/etcd
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: chart-values-rke2-coredns
    - configMap:
        defaultMode: 420
        name: chart-content-rke2-coredns
      name: content
    - name: kube-api-access-sl96p
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:57Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:52Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:55Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:55Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:52Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://60c95f51aad28e93e0f0433093ad8272ab50cebdcaa8fea42f717734bb80e43f
      image: docker.io/rancher/klipper-helm:v0.9.10-build20251111
      imageID: sha256:1d0854c41ff27a347388550f509052958313f7cb0b013a73bc0f5cf3978e4053
      lastState: {}
      name: helm
      ready: false
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://60c95f51aad28e93e0f0433093ad8272ab50cebdcaa8fea42f717734bb80e43f
          exitCode: 0
          finishedAt: "2025-12-30T21:46:54Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-12-30T21:46:53Z"
      user:
        linux:
          gid: 1000
          supplementalGroups:
          - 1000
          uid: 1000
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sl96p
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Succeeded
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: BestEffort
    startTime: "2025-12-30T21:46:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 71d2c335dee51b8a8afc7dc6f30297bb31beabb104346767b255145215b540f7
      cni.projectcalico.org/podIP: "null"
      cni.projectcalico.org/podIPs: "null"
      helmcharts.helm.cattle.io/configHash: SHA256=3324726566A40F412BBFE230BFADC8DE67A35A5BF43DA7DEF03B1454BCC9B92B
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.78"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2025-12-30T21:50:36Z"
    generateName: helm-install-rke2-ingress-nginx-
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: c9dfd56b-f43d-4fe8-8e90-321772f81235
      batch.kubernetes.io/job-name: helm-install-rke2-ingress-nginx
      controller-uid: c9dfd56b-f43d-4fe8-8e90-321772f81235
      helmcharts.helm.cattle.io/chart: rke2-ingress-nginx
      job-name: helm-install-rke2-ingress-nginx
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:helmcharts.helm.cattle.io/configHash: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:batch.kubernetes.io/controller-uid: {}
            f:batch.kubernetes.io/job-name: {}
            f:controller-uid: {}
            f:helmcharts.helm.cattle.io/chart: {}
            f:job-name: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"c9dfd56b-f43d-4fe8-8e90-321772f81235"}: {}
        f:spec:
          f:containers:
            k:{"name":"helm"}:
              .: {}
              f:args: {}
              f:env:
                .: {}
                k:{"name":"AUTH_PASS_CREDENTIALS"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CHART"}:
                  .: {}
                  f:name: {}
                k:{"name":"CHART_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FAILURE_POLICY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_DRIVER"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_VERSION"}:
                  .: {}
                  f:name: {}
                k:{"name":"INSECURE_SKIP_TLS_VERIFY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NAME"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"PLAIN_HTTP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"REPO"}:
                  .: {}
                  f:name: {}
                k:{"name":"TARGET_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"VERSION"}:
                  .: {}
                  f:name: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/chart"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.cache"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.helm"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/tmp"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext:
            .: {}
            f:runAsNonRoot: {}
            f:seccompProfile:
              .: {}
              f:type: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:volumes:
            .: {}
            k:{"name":"content"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"klipper-cache"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-config"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-helm"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"tmp"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"values"}:
              .: {}
              f:name: {}
              f:projected:
                .: {}
                f:defaultMode: {}
                f:sources: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:50:36Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:50:36Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:50:52Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:50:52Z"
    name: helm-install-rke2-ingress-nginx-szhr7
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-rke2-ingress-nginx
      uid: c9dfd56b-f43d-4fe8-8e90-321772f81235
    resourceVersion: "8046"
    uid: f2670e09-d894-4dff-911e-511b06754917
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.clusterCIDR=10.52.0.0/16
      - --set-string
      - global.clusterCIDRv4=10.52.0.0/16
      - --set-string
      - global.clusterDNS=10.53.0.10
      - --set-string
      - global.clusterDomain=cluster.local
      - --set-string
      - global.rke2DataDir=/var/lib/rancher/rke2
      - --set-string
      - global.serviceCIDR=10.53.0.0/16
      - --set-string
      - global.systemDefaultIngressClass=ingress-nginx
      env:
      - name: NAME
        value: rke2-ingress-nginx
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.10-build20251111
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pt2mg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-rke2-ingress-nginx
    serviceAccountName: helm-rke2-ingress-nginx
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      projected:
        defaultMode: 420
        sources:
        - secret:
            items:
            - key: HelmChartConfigValuesContent
              path: values-1-000-HelmChartConfig-ValuesContent.yaml
            name: chart-values-rke2-ingress-nginx
    - configMap:
        defaultMode: 420
        name: chart-content-rke2-ingress-nginx
      name: content
    - name: kube-api-access-pt2mg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:50:52Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:50:36Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:50:51Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:50:51Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:50:36Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://98e13f32c0c5a5f49ee5e145a2789a79e42b18761a4ef656ab1008af8254c055
      image: docker.io/rancher/klipper-helm:v0.9.10-build20251111
      imageID: sha256:1d0854c41ff27a347388550f509052958313f7cb0b013a73bc0f5cf3978e4053
      lastState: {}
      name: helm
      ready: false
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://98e13f32c0c5a5f49ee5e145a2789a79e42b18761a4ef656ab1008af8254c055
          exitCode: 0
          finishedAt: "2025-12-30T21:50:50Z"
          message: |
            Upgrading helm chart
          reason: Completed
          startedAt: "2025-12-30T21:50:38Z"
      user:
        linux:
          gid: 1000
          supplementalGroups:
          - 1000
          uid: 1000
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pt2mg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-12-30T21:50:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: d5da5056d7eca034c5094a322e8a904f3e4e35e141db463464d7684380a68a41
      cni.projectcalico.org/podIP: "null"
      cni.projectcalico.org/podIPs: "null"
      helmcharts.helm.cattle.io/configHash: SHA256=4C8F30AADDDBCA806321E874F2BCA6079760BF5944927714551AF833EEC625C6
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.3"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2025-12-30T21:46:52Z"
    generateName: helm-install-rke2-metrics-server-
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: a62c4bd7-b2be-442a-b063-53dcf3f67638
      batch.kubernetes.io/job-name: helm-install-rke2-metrics-server
      controller-uid: a62c4bd7-b2be-442a-b063-53dcf3f67638
      helmcharts.helm.cattle.io/chart: rke2-metrics-server
      job-name: helm-install-rke2-metrics-server
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:helmcharts.helm.cattle.io/configHash: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:batch.kubernetes.io/controller-uid: {}
            f:batch.kubernetes.io/job-name: {}
            f:controller-uid: {}
            f:helmcharts.helm.cattle.io/chart: {}
            f:job-name: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"a62c4bd7-b2be-442a-b063-53dcf3f67638"}: {}
        f:spec:
          f:containers:
            k:{"name":"helm"}:
              .: {}
              f:args: {}
              f:env:
                .: {}
                k:{"name":"AUTH_PASS_CREDENTIALS"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CHART"}:
                  .: {}
                  f:name: {}
                k:{"name":"CHART_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FAILURE_POLICY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_DRIVER"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_VERSION"}:
                  .: {}
                  f:name: {}
                k:{"name":"INSECURE_SKIP_TLS_VERIFY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NAME"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"PLAIN_HTTP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"REPO"}:
                  .: {}
                  f:name: {}
                k:{"name":"TARGET_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"VERSION"}:
                  .: {}
                  f:name: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/chart"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.cache"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.helm"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/tmp"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext:
            .: {}
            f:runAsNonRoot: {}
            f:seccompProfile:
              .: {}
              f:type: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:volumes:
            .: {}
            k:{"name":"content"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"klipper-cache"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-config"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-helm"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"tmp"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"values"}:
              .: {}
              f:name: {}
              f:projected:
                .: {}
                f:defaultMode: {}
                f:sources: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:46:52Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
      manager: kube-scheduler
      operation: Update
      subresource: status
      time: "2025-12-30T21:46:52Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:11Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:13Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:13Z"
    name: helm-install-rke2-metrics-server-m2f5k
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-rke2-metrics-server
      uid: a62c4bd7-b2be-442a-b063-53dcf3f67638
    resourceVersion: "8045"
    uid: c602a921-5d17-4097-91ce-472a9bdea79f
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.clusterCIDR=10.52.0.0/16
      - --set-string
      - global.clusterCIDRv4=10.52.0.0/16
      - --set-string
      - global.clusterDNS=10.53.0.10
      - --set-string
      - global.clusterDomain=cluster.local
      - --set-string
      - global.rke2DataDir=/var/lib/rancher/rke2
      - --set-string
      - global.serviceCIDR=10.53.0.0/16
      - --set-string
      - global.systemDefaultIngressClass=ingress-nginx
      env:
      - name: NAME
        value: rke2-metrics-server
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.10-build20251111
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ngtfw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-rke2-metrics-server
    serviceAccountName: helm-rke2-metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: chart-values-rke2-metrics-server
    - configMap:
        defaultMode: 420
        name: chart-content-rke2-metrics-server
      name: content
    - name: kube-api-access-ngtfw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:47:13Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://40d37218d1119dd5f4b9d1b64b6775e44c5d853ae8d2e4f3b350fd65298f78ff
      image: docker.io/rancher/klipper-helm:v0.9.10-build20251111
      imageID: sha256:1d0854c41ff27a347388550f509052958313f7cb0b013a73bc0f5cf3978e4053
      lastState: {}
      name: helm
      ready: false
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://40d37218d1119dd5f4b9d1b64b6775e44c5d853ae8d2e4f3b350fd65298f78ff
          exitCode: 0
          finishedAt: "2025-12-30T21:47:11Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-12-30T21:47:11Z"
      user:
        linux:
          gid: 1000
          supplementalGroups:
          - 1000
          uid: 1000
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ngtfw
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-12-30T21:46:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=2715D14EDE86447D46BB2C554FD8A23D8A7B4C7A54223A386F764C4D8860D67C
    creationTimestamp: "2025-12-30T21:46:52Z"
    generateName: helm-install-rke2-multus-
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 57ccba6b-f6c5-4742-be25-0dee642c03ad
      batch.kubernetes.io/job-name: helm-install-rke2-multus
      controller-uid: 57ccba6b-f6c5-4742-be25-0dee642c03ad
      helmcharts.helm.cattle.io/chart: rke2-multus
      job-name: helm-install-rke2-multus
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:helmcharts.helm.cattle.io/configHash: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:batch.kubernetes.io/controller-uid: {}
            f:batch.kubernetes.io/job-name: {}
            f:controller-uid: {}
            f:helmcharts.helm.cattle.io/chart: {}
            f:job-name: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"57ccba6b-f6c5-4742-be25-0dee642c03ad"}: {}
        f:spec:
          f:containers:
            k:{"name":"helm"}:
              .: {}
              f:args: {}
              f:env:
                .: {}
                k:{"name":"AUTH_PASS_CREDENTIALS"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"BOOTSTRAP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CHART"}:
                  .: {}
                  f:name: {}
                k:{"name":"CHART_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FAILURE_POLICY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_DRIVER"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_VERSION"}:
                  .: {}
                  f:name: {}
                k:{"name":"INSECURE_SKIP_TLS_VERIFY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"KUBERNETES_SERVICE_HOST"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"KUBERNETES_SERVICE_PORT"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NAME"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"PLAIN_HTTP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"REPO"}:
                  .: {}
                  f:name: {}
                k:{"name":"TARGET_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"VERSION"}:
                  .: {}
                  f:name: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/chart"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.cache"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.helm"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/tmp"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeSelector: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext:
            .: {}
            f:runAsNonRoot: {}
            f:seccompProfile:
              .: {}
              f:type: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"content"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"klipper-cache"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-config"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-helm"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"tmp"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"values"}:
              .: {}
              f:name: {}
              f:projected:
                .: {}
                f:defaultMode: {}
                f:sources: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:46:52Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              f:observedGeneration: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:observedGeneration: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:46:55Z"
    name: helm-install-rke2-multus-ckz6t
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-rke2-multus
      uid: 57ccba6b-f6c5-4742-be25-0dee642c03ad
    resourceVersion: "753"
    uid: 26862891-b63f-46a9-bd8d-3b2a3a746ffc
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.clusterCIDR=10.52.0.0/16
      - --set-string
      - global.clusterCIDRv4=10.52.0.0/16
      - --set-string
      - global.clusterDNS=10.53.0.10
      - --set-string
      - global.clusterDomain=cluster.local
      - --set-string
      - global.rke2DataDir=/var/lib/rancher/rke2
      - --set-string
      - global.serviceCIDR=10.53.0.0/16
      - --set-string
      - global.systemDefaultIngressClass=ingress-nginx
      env:
      - name: NAME
        value: rke2-multus
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: KUBERNETES_SERVICE_HOST
        value: 127.0.0.1
      - name: KUBERNETES_SERVICE_PORT
        value: "6443"
      - name: BOOTSTRAP
        value: "true"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.10-build20251111
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x6hq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
      node-role.kubernetes.io/control-plane: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-rke2-multus
    serviceAccountName: helm-rke2-multus
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Equal
      value: "true"
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node-role.kubernetes.io/etcd
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: chart-values-rke2-multus
    - configMap:
        defaultMode: 420
        name: chart-content-rke2-multus
      name: content
    - name: kube-api-access-x6hq5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:55Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:52Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:52Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:52Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:52Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://78cbfe5f33ceb2c46d9c062f81d939b22c7807564486fbd9205e61e347d1d251
      image: docker.io/rancher/klipper-helm:v0.9.10-build20251111
      imageID: sha256:1d0854c41ff27a347388550f509052958313f7cb0b013a73bc0f5cf3978e4053
      lastState: {}
      name: helm
      ready: false
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://78cbfe5f33ceb2c46d9c062f81d939b22c7807564486fbd9205e61e347d1d251
          exitCode: 0
          finishedAt: "2025-12-30T21:46:53Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-12-30T21:46:53Z"
      user:
        linux:
          gid: 1000
          supplementalGroups:
          - 1000
          uid: 1000
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x6hq5
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Succeeded
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: BestEffort
    startTime: "2025-12-30T21:46:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: add15c52b1b5068ef05868fd94ae25c9681f7460f182ab3670e5b6b4266080d3
      cni.projectcalico.org/podIP: "null"
      cni.projectcalico.org/podIPs: "null"
      helmcharts.helm.cattle.io/configHash: SHA256=3554377A452839F582C279735186093D57EA6700DB4076BE8DBC4CA20EBD3AF5
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.6"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2025-12-30T21:46:52Z"
    generateName: helm-install-rke2-runtimeclasses-
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 9dd15d84-4ce0-4074-89d5-b9742001525a
      batch.kubernetes.io/job-name: helm-install-rke2-runtimeclasses
      controller-uid: 9dd15d84-4ce0-4074-89d5-b9742001525a
      helmcharts.helm.cattle.io/chart: rke2-runtimeclasses
      job-name: helm-install-rke2-runtimeclasses
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:helmcharts.helm.cattle.io/configHash: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:batch.kubernetes.io/controller-uid: {}
            f:batch.kubernetes.io/job-name: {}
            f:controller-uid: {}
            f:helmcharts.helm.cattle.io/chart: {}
            f:job-name: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"9dd15d84-4ce0-4074-89d5-b9742001525a"}: {}
        f:spec:
          f:containers:
            k:{"name":"helm"}:
              .: {}
              f:args: {}
              f:env:
                .: {}
                k:{"name":"AUTH_PASS_CREDENTIALS"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CHART"}:
                  .: {}
                  f:name: {}
                k:{"name":"CHART_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FAILURE_POLICY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_DRIVER"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_VERSION"}:
                  .: {}
                  f:name: {}
                k:{"name":"INSECURE_SKIP_TLS_VERIFY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NAME"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"PLAIN_HTTP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"REPO"}:
                  .: {}
                  f:name: {}
                k:{"name":"TARGET_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"VERSION"}:
                  .: {}
                  f:name: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/chart"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.cache"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/home/klipper-helm/.helm"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/tmp"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext:
            .: {}
            f:runAsNonRoot: {}
            f:seccompProfile:
              .: {}
              f:type: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:volumes:
            .: {}
            k:{"name":"content"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"klipper-cache"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-config"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"klipper-helm"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"tmp"}:
              .: {}
              f:emptyDir:
                .: {}
                f:medium: {}
              f:name: {}
            k:{"name":"values"}:
              .: {}
              f:name: {}
              f:projected:
                .: {}
                f:defaultMode: {}
                f:sources: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:46:52Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
      manager: kube-scheduler
      operation: Update
      subresource: status
      time: "2025-12-30T21:46:52Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:12Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:14Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:14Z"
    name: helm-install-rke2-runtimeclasses-tvtq5
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-rke2-runtimeclasses
      uid: 9dd15d84-4ce0-4074-89d5-b9742001525a
    resourceVersion: "8076"
    uid: d19fa0c6-43d9-402a-abc8-ea08a3e64477
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.clusterCIDR=10.52.0.0/16
      - --set-string
      - global.clusterCIDRv4=10.52.0.0/16
      - --set-string
      - global.clusterDNS=10.53.0.10
      - --set-string
      - global.clusterDomain=cluster.local
      - --set-string
      - global.rke2DataDir=/var/lib/rancher/rke2
      - --set-string
      - global.serviceCIDR=10.53.0.0/16
      - --set-string
      - global.systemDefaultIngressClass=ingress-nginx
      env:
      - name: NAME
        value: rke2-runtimeclasses
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.10-build20251111
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j9lms
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-rke2-runtimeclasses
    serviceAccountName: helm-rke2-runtimeclasses
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: chart-values-rke2-runtimeclasses
    - configMap:
        defaultMode: 420
        name: chart-content-rke2-runtimeclasses
      name: content
    - name: kube-api-access-j9lms
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:47:14Z"
      observedGeneration: 1
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cccee43fdd0c8a24e769a8e83a58de5b3f8f00a55e39c2368ebc202561f2e576
      image: docker.io/rancher/klipper-helm:v0.9.10-build20251111
      imageID: sha256:1d0854c41ff27a347388550f509052958313f7cb0b013a73bc0f5cf3978e4053
      lastState: {}
      name: helm
      ready: false
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://cccee43fdd0c8a24e769a8e83a58de5b3f8f00a55e39c2368ebc202561f2e576
          exitCode: 0
          finishedAt: "2025-12-30T21:47:12Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-12-30T21:47:12Z"
      user:
        linux:
          gid: 1000
          supplementalGroups:
          - 1000
          uid: 1000
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j9lms
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-12-30T21:46:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: c8a6bdd733d69cbd72bc6a261bfb9596
      kubernetes.io/config.mirror: c8a6bdd733d69cbd72bc6a261bfb9596
      kubernetes.io/config.seen: "2025-12-30T21:46:36.057589241Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-12-30T21:46:43Z"
    generation: 1
    labels:
      component: kube-apiserver
      tier: control-plane
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubernetes.io/config.hash: {}
            f:kubernetes.io/config.mirror: {}
            f:kubernetes.io/config.seen: {}
            f:kubernetes.io/config.source: {}
          f:labels:
            .: {}
            f:component: {}
            f:tier: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"75908add-436d-4a4a-ac7f-91584193f4b7"}: {}
        f:spec:
          f:containers:
            k:{"name":"kube-apiserver"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"FILE_HASH"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:exec:
                  .: {}
                  f:command: {}
                f:failureThreshold: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":6443,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
              f:readinessProbe:
                .: {}
                f:exec:
                  .: {}
                  f:command: {}
                f:failureThreshold: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:startupProbe:
                .: {}
                f:exec:
                  .: {}
                  f:command: {}
                f:failureThreshold: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/ca-certificates"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/etc/rancher/rke2/config.yaml.d/92-harvester-kube-audit-policy.yaml"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/etc/rancher/rke2/rke2-pss.yaml"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/etc/ssl/certs"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/db/etcd/name"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/etc/egress-selector-config.yaml"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/logs"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-auth-proxy.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-auth-proxy.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/etcd/client.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/etcd/client.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/request-header-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/server-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/service.current.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/service.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/serving-kube-apiserver.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeName: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"dir0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"dir1"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"dir2"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"dir3"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file1"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file2"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file3"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file4"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file5"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file6"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file7"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file8"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file9"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file10"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file11"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file12"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file13"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file14"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file15"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file16"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file17"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
      manager: kubelet
      operation: Update
      time: "2025-12-30T21:46:43Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:51:59Z"
    name: kube-apiserver-isim-dev
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: isim-dev
      uid: 75908add-436d-4a4a-ac7f-91584193f4b7
    resourceVersion: "8167"
    uid: b9bd3f03-d844-4936-8a09-1cb7e4f56aae
  spec:
    containers:
    - args:
      - --admission-control-config-file=/etc/rancher/rke2/rke2-pss.yaml
      - --audit-policy-file=/etc/rancher/rke2/config.yaml.d/92-harvester-kube-audit-policy.yaml
      - --audit-log-maxage=30
      - --audit-log-maxbackup=10
      - --audit-log-maxsize=100
      - --audit-log-path=/var/lib/rancher/rke2/server/logs/audit.log
      - --allow-privileged=true
      - --anonymous-auth=false
      - --api-audiences=https://kubernetes.default.svc.cluster.local,rke2
      - --authorization-mode=Node,RBAC
      - --bind-address=0.0.0.0
      - --cert-dir=/var/lib/rancher/rke2/server/tls/temporary-certs
      - --client-ca-file=/var/lib/rancher/rke2/server/tls/client-ca.crt
      - --egress-selector-config-file=/var/lib/rancher/rke2/server/etc/egress-selector-config.yaml
      - --enable-admission-plugins=NodeRestriction
      - --enable-aggregator-routing=true
      - --enable-bootstrap-token-auth=true
      - --encryption-provider-config=/var/lib/rancher/rke2/server/cred/encryption-config.json
      - --encryption-provider-config-automatic-reload=true
      - --etcd-cafile=/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt
      - --etcd-certfile=/var/lib/rancher/rke2/server/tls/etcd/client.crt
      - --etcd-keyfile=/var/lib/rancher/rke2/server/tls/etcd/client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --kubelet-certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt
      - --kubelet-client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt
      - --kubelet-client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --profiling=false
      - --proxy-client-cert-file=/var/lib/rancher/rke2/server/tls/client-auth-proxy.crt
      - --proxy-client-key-file=/var/lib/rancher/rke2/server/tls/client-auth-proxy.key
      - --requestheader-allowed-names=system:auth-proxy
      - --requestheader-client-ca-file=/var/lib/rancher/rke2/server/tls/request-header-ca.crt
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --requestheader-group-headers=X-Remote-Group
      - --requestheader-username-headers=X-Remote-User
      - --secure-port=6443
      - --service-account-issuer=https://kubernetes.default.svc.cluster.local
      - --service-account-key-file=/var/lib/rancher/rke2/server/tls/service.key
      - --service-account-signing-key-file=/var/lib/rancher/rke2/server/tls/service.current.key
      - --service-cluster-ip-range=10.53.0.0/16
      - --service-node-port-range=30000-32767
      - --storage-backend=etcd3
      - --tls-cert-file=/var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt
      - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
      - --tls-private-key-file=/var/lib/rancher/rke2/server/tls/serving-kube-apiserver.key
      command:
      - kube-apiserver
      env:
      - name: FILE_HASH
        value: eb2f2dcf4930c12bd0b62844f76da88748c80b98f7166012d1e2a3d4db11b7e6
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      image: index.docker.io/rancher/hardened-kubernetes:v1.34.2-rke2r1-build20251112
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - kubectl
          - get
          - --server=https://localhost:6443/
          - --client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt
          - --client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key
          - --certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt
          - --raw=/livez
        failureThreshold: 8
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      ports:
      - containerPort: 6443
        hostPort: 6443
        name: apiserver
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - kubectl
          - get
          - --server=https://localhost:6443/
          - --client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt
          - --client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key
          - --certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt
          - --raw=/readyz
        failureThreshold: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 250m
          memory: 1Gi
      securityContext:
        privileged: false
      startupProbe:
        exec:
          command:
          - kubectl
          - get
          - --server=https://localhost:6443/
          - --client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt
          - --client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key
          - --certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt
          - --raw=/livez
        failureThreshold: 24
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: dir0
      - mountPath: /etc/ca-certificates
        name: dir1
      - mountPath: /var/lib/rancher/rke2/server/logs
        name: dir2
      - mountPath: /var/lib/rancher/rke2/server
        name: dir3
      - mountPath: /var/lib/rancher/rke2/server/db/etcd/name
        name: file0
        readOnly: true
      - mountPath: /etc/rancher/rke2/config.yaml.d/92-harvester-kube-audit-policy.yaml
        name: file1
        readOnly: true
      - mountPath: /etc/rancher/rke2/rke2-pss.yaml
        name: file2
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/etc/egress-selector-config.yaml
        name: file3
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-auth-proxy.crt
        name: file4
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-auth-proxy.key
        name: file5
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-ca.crt
        name: file6
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt
        name: file7
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-kube-apiserver.key
        name: file8
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/etcd/client.crt
        name: file9
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/etcd/client.key
        name: file10
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt
        name: file11
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/request-header-ca.crt
        name: file12
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.crt
        name: file13
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/service.current.key
        name: file14
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/service.key
        name: file15
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt
        name: file16
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.key
        name: file17
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 2e+09
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: dir0
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: dir1
    - hostPath:
        path: /var/lib/rancher/rke2/server/logs
        type: DirectoryOrCreate
      name: dir2
    - hostPath:
        path: /var/lib/rancher/rke2/server
        type: DirectoryOrCreate
      name: dir3
    - hostPath:
        path: /var/lib/rancher/rke2/server/db/etcd/name
        type: File
      name: file0
    - hostPath:
        path: /etc/rancher/rke2/config.yaml.d/92-harvester-kube-audit-policy.yaml
        type: File
      name: file1
    - hostPath:
        path: /etc/rancher/rke2/rke2-pss.yaml
        type: File
      name: file2
    - hostPath:
        path: /var/lib/rancher/rke2/server/etc/egress-selector-config.yaml
        type: File
      name: file3
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-auth-proxy.crt
        type: File
      name: file4
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-auth-proxy.key
        type: File
      name: file5
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-ca.crt
        type: File
      name: file6
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt
        type: File
      name: file7
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-kube-apiserver.key
        type: File
      name: file8
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/etcd/client.crt
        type: File
      name: file9
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/etcd/client.key
        type: File
      name: file10
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt
        type: File
      name: file11
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/request-header-ca.crt
        type: File
      name: file12
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/server-ca.crt
        type: File
      name: file13
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/service.current.key
        type: File
      name: file14
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/service.key
        type: File
      name: file15
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt
        type: File
      name: file16
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.key
        type: File
      name: file17
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 250m
        memory: 1Gi
      containerID: containerd://84a48de9fe0c897509eddcb3c543d38ceafbe413128064ba0c606d273a7912e3
      image: docker.io/rancher/hardened-kubernetes:v1.34.2-rke2r1-build20251112
      imageID: sha256:7b491b625ab3a5905fb43f5778a0c0066493b638295ee423f02c55d17920e384
      lastState: {}
      name: kube-apiserver
      ready: true
      resources:
        requests:
          cpu: 250m
          memory: 1Gi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:46:36Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    phase: Running
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: Burstable
    startTime: "2025-12-30T21:51:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: f0bd721e2299942d94fa204c84472ca7
      kubernetes.io/config.mirror: f0bd721e2299942d94fa204c84472ca7
      kubernetes.io/config.seen: "2025-12-30T21:52:12.077340743Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-12-30T21:52:26Z"
    generation: 1
    labels:
      component: kube-controller-manager
      tier: control-plane
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubernetes.io/config.hash: {}
            f:kubernetes.io/config.mirror: {}
            f:kubernetes.io/config.seen: {}
            f:kubernetes.io/config.source: {}
          f:labels:
            .: {}
            f:component: {}
            f:tier: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"75908add-436d-4a4a-ac7f-91584193f4b7"}: {}
        f:spec:
          f:containers:
            k:{"name":"kube-controller-manager"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"FILE_HASH"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":10257,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:startupProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/ca-certificates"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/etc/ssl/certs"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/cred/controller.kubeconfig"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/db/etcd/name"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-ca.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-ca.nochain.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-controller.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-controller.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/kube-controller-manager"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/server-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/server-ca.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/server-ca.nochain.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/service.current.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeName: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"dir0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"dir1"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"extra-mount-0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file1"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file2"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file3"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file4"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file5"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file6"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file7"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file8"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file9"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file10"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file11"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
      manager: kubelet
      operation: Update
      time: "2025-12-30T21:52:26Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:52:46Z"
    name: kube-controller-manager-isim-dev
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: isim-dev
      uid: 75908add-436d-4a4a-ac7f-91584193f4b7
    resourceVersion: "8913"
    uid: b819db41-9107-429e-9416-824bf3a5d345
  spec:
    containers:
    - args:
      - --permit-port-sharing=true
      - --flex-volume-plugin-dir=/var/lib/kubelet/volumeplugins
      - --terminated-pod-gc-threshold=1000
      - --allocate-node-cidrs=true
      - --authentication-kubeconfig=/var/lib/rancher/rke2/server/cred/controller.kubeconfig
      - --authorization-kubeconfig=/var/lib/rancher/rke2/server/cred/controller.kubeconfig
      - --bind-address=127.0.0.1
      - --cert-dir=/var/lib/rancher/rke2/server/tls/kube-controller-manager
      - --cluster-cidr=10.52.0.0/16
      - --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/rke2/server/tls/client-ca.nochain.crt
      - --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/rke2/server/tls/client-ca.key
      - --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/rke2/server/tls/client-ca.nochain.crt
      - --cluster-signing-kubelet-client-key-file=/var/lib/rancher/rke2/server/tls/client-ca.key
      - --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/rke2/server/tls/server-ca.nochain.crt
      - --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/rke2/server/tls/server-ca.key
      - --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/rke2/server/tls/server-ca.nochain.crt
      - --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/rke2/server/tls/server-ca.key
      - --configure-cloud-routes=false
      - --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle
      - --kubeconfig=/var/lib/rancher/rke2/server/cred/controller.kubeconfig
      - --profiling=false
      - --root-ca-file=/var/lib/rancher/rke2/server/tls/server-ca.crt
      - --secure-port=10257
      - --service-account-private-key-file=/var/lib/rancher/rke2/server/tls/service.current.key
      - --service-cluster-ip-range=10.53.0.0/16
      - --tls-cert-file=/var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.crt
      - --tls-private-key-file=/var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.key
      - --use-service-account-credentials=true
      command:
      - kube-controller-manager
      env:
      - name: FILE_HASH
        value: cf54bbeec0f21837d82bbfcea25435c0347595a8f10ebf9cd6392e0e72cfd7eb
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      image: index.docker.io/rancher/hardened-kubernetes:v1.34.2-rke2r1-build20251112
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: localhost
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      ports:
      - containerPort: 10257
        hostPort: 10257
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 200m
          memory: 256Mi
      securityContext:
        privileged: false
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: localhost
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: dir0
      - mountPath: /etc/ca-certificates
        name: dir1
      - mountPath: /var/lib/rancher/rke2/server/db/etcd/name
        name: file0
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/cred/controller.kubeconfig
        name: file1
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-ca.key
        name: file2
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-ca.nochain.crt
        name: file3
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-controller.crt
        name: file4
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-controller.key
        name: file5
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.crt
        name: file6
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.key
        name: file7
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.crt
        name: file8
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.key
        name: file9
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.nochain.crt
        name: file10
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/service.current.key
        name: file11
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-controller-manager
        name: extra-mount-0
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 2e+09
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: dir0
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: dir1
    - hostPath:
        path: /var/lib/rancher/rke2/server/db/etcd/name
        type: File
      name: file0
    - hostPath:
        path: /var/lib/rancher/rke2/server/cred/controller.kubeconfig
        type: File
      name: file1
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-ca.key
        type: File
      name: file2
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-ca.nochain.crt
        type: File
      name: file3
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-controller.crt
        type: File
      name: file4
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-controller.key
        type: File
      name: file5
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.crt
        type: File
      name: file6
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.key
        type: File
      name: file7
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/server-ca.crt
        type: File
      name: file8
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/server-ca.key
        type: File
      name: file9
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/server-ca.nochain.crt
        type: File
      name: file10
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/service.current.key
        type: File
      name: file11
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/kube-controller-manager
        type: Directory
      name: extra-mount-0
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:27Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 200m
        memory: 256Mi
      containerID: containerd://acf79d8b26db406071ec2456027dc4420fe7a0871c153303d38e4fb8580fdcd8
      image: docker.io/rancher/hardened-kubernetes:v1.34.2-rke2r1-build20251112
      imageID: sha256:7b491b625ab3a5905fb43f5778a0c0066493b638295ee423f02c55d17920e384
      lastState: {}
      name: kube-controller-manager
      ready: true
      resources:
        requests:
          cpu: 200m
          memory: 256Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:52:27Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: dir0
      - mountPath: /etc/ca-certificates
        name: dir1
      - mountPath: /var/lib/rancher/rke2/server/db/etcd/name
        name: file0
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/cred/controller.kubeconfig
        name: file1
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/client-ca.key
        name: file2
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/client-ca.nochain.crt
        name: file3
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/client-controller.crt
        name: file4
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/client-controller.key
        name: file5
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.crt
        name: file6
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.key
        name: file7
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.crt
        name: file8
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.key
        name: file9
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.nochain.crt
        name: file10
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/service.current.key
        name: file11
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-controller-manager
        name: extra-mount-0
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    phase: Running
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: Burstable
    startTime: "2025-12-30T21:51:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 836fde9f5260e96b2ca014295ff586d0
      kubernetes.io/config.mirror: 836fde9f5260e96b2ca014295ff586d0
      kubernetes.io/config.seen: "2025-12-30T21:51:54.863792137Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-12-30T21:51:56Z"
    generation: 1
    labels:
      component: kube-proxy
      tier: control-plane
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubernetes.io/config.hash: {}
            f:kubernetes.io/config.mirror: {}
            f:kubernetes.io/config.seen: {}
            f:kubernetes.io/config.source: {}
          f:labels:
            .: {}
            f:component: {}
            f:tier: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"75908add-436d-4a4a-ac7f-91584193f4b7"}: {}
        f:spec:
          f:containers:
            k:{"name":"kube-proxy"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"FILE_HASH"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":10256,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/var/lib/rancher/rke2/agent/client-kube-proxy.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/agent/client-kube-proxy.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/agent/kubeproxy.kubeconfig"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/agent/server-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeName: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"file0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file1"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file2"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file3"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
      manager: kubelet
      operation: Update
      time: "2025-12-30T21:51:56Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:51:58Z"
    name: kube-proxy-isim-dev
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: isim-dev
      uid: 75908add-436d-4a4a-ac7f-91584193f4b7
    resourceVersion: "8092"
    uid: 694ef15a-c3be-41bb-ac9e-69f61ae802be
  spec:
    containers:
    - args:
      - --cluster-cidr=10.52.0.0/16
      - --conntrack-max-per-core=0
      - --conntrack-tcp-timeout-close-wait=0s
      - --conntrack-tcp-timeout-established=0s
      - --healthz-bind-address=127.0.0.1
      - --hostname-override=isim-dev
      - --kubeconfig=/var/lib/rancher/rke2/agent/kubeproxy.kubeconfig
      - --proxy-mode=iptables
      command:
      - kube-proxy
      env:
      - name: FILE_HASH
        value: e038a52279e2ef4a1196782ef00c876f434335ced5166ece90973d773eecb928
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      image: index.docker.io/rancher/hardened-kubernetes:v1.34.2-rke2r1-build20251112
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: localhost
          path: /livez
          port: 10256
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-proxy
      ports:
      - containerPort: 10256
        hostPort: 10256
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 250m
          memory: 128Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rancher/rke2/agent/client-kube-proxy.crt
        name: file0
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/agent/client-kube-proxy.key
        name: file1
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/agent/kubeproxy.kubeconfig
        name: file2
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/agent/server-ca.crt
        name: file3
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 2e+09
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/rancher/rke2/agent/client-kube-proxy.crt
        type: File
      name: file0
    - hostPath:
        path: /var/lib/rancher/rke2/agent/client-kube-proxy.key
        type: File
      name: file1
    - hostPath:
        path: /var/lib/rancher/rke2/agent/kubeproxy.kubeconfig
        type: File
      name: file2
    - hostPath:
        path: /var/lib/rancher/rke2/agent/server-ca.crt
        type: File
      name: file3
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:56Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 250m
        memory: 128Mi
      containerID: containerd://de01daeb89b38e2e4081d2551eb39fb848579d6569f340a3d388a47d738b1e24
      image: docker.io/rancher/hardened-kubernetes:v1.34.2-rke2r1-build20251112
      imageID: sha256:7b491b625ab3a5905fb43f5778a0c0066493b638295ee423f02c55d17920e384
      lastState: {}
      name: kube-proxy
      ready: true
      resources:
        requests:
          cpu: 250m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:51:55Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/rancher/rke2/agent/client-kube-proxy.crt
        name: file0
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/agent/client-kube-proxy.key
        name: file1
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/agent/kubeproxy.kubeconfig
        name: file2
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/agent/server-ca.crt
        name: file3
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    phase: Running
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: Burstable
    startTime: "2025-12-30T21:51:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 94ea056c930d628eb27bcc0d97a2eaf0
      kubernetes.io/config.mirror: 94ea056c930d628eb27bcc0d97a2eaf0
      kubernetes.io/config.seen: "2025-12-30T21:52:12.077724040Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-12-30T21:52:24Z"
    generation: 1
    labels:
      component: kube-scheduler
      tier: control-plane
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubernetes.io/config.hash: {}
            f:kubernetes.io/config.mirror: {}
            f:kubernetes.io/config.seen: {}
            f:kubernetes.io/config.source: {}
          f:labels:
            .: {}
            f:component: {}
            f:tier: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"75908add-436d-4a4a-ac7f-91584193f4b7"}: {}
        f:spec:
          f:containers:
            k:{"name":"kube-scheduler"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"FILE_HASH"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":10259,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
              f:readinessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:startupProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/db/etcd/name"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-scheduler.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/client-scheduler.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/kube-scheduler"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.key"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/rancher/rke2/server/tls/server-ca.crt"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeName: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"extra-mount-0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file0"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file1"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file2"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file3"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file4"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file5"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"file6"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
      manager: kubelet
      operation: Update
      time: "2025-12-30T21:52:24Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:52:44Z"
    name: kube-scheduler-isim-dev
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: isim-dev
      uid: 75908add-436d-4a4a-ac7f-91584193f4b7
    resourceVersion: "8884"
    uid: b6f5e5b0-ff35-40f5-910e-20fc23b13c10
  spec:
    containers:
    - args:
      - --permit-port-sharing=true
      - --authentication-kubeconfig=/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
      - --authorization-kubeconfig=/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
      - --bind-address=127.0.0.1
      - --cert-dir=/var/lib/rancher/rke2/server/tls/kube-scheduler
      - --kubeconfig=/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
      - --profiling=false
      - --secure-port=10259
      - --tls-cert-file=/var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.crt
      - --tls-private-key-file=/var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.key
      command:
      - kube-scheduler
      env:
      - name: FILE_HASH
        value: 09b8a3f9d2cd55c1bcc1b965976e8267324e159d64fc835d893e96bfc0e01ad7
      - name: NO_PROXY
        value: .svc,.cluster.local,10.52.0.0/16,10.53.0.0/16
      image: index.docker.io/rancher/hardened-kubernetes:v1.34.2-rke2r1-build20251112
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: localhost
          path: /livez
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      ports:
      - containerPort: 10259
        hostPort: 10259
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /readyz
          port: 10259
          scheme: HTTPS
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        privileged: false
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: localhost
          path: /livez
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rancher/rke2/server/db/etcd/name
        name: file0
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
        name: file1
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-scheduler.crt
        name: file2
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/client-scheduler.key
        name: file3
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.crt
        name: file4
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.key
        name: file5
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.crt
        name: file6
        readOnly: true
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-scheduler
        name: extra-mount-0
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 2e+09
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/rancher/rke2/server/db/etcd/name
        type: File
      name: file0
    - hostPath:
        path: /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
        type: File
      name: file1
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-scheduler.crt
        type: File
      name: file2
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/client-scheduler.key
        type: File
      name: file3
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.crt
        type: File
      name: file4
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.key
        type: File
      name: file5
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/server-ca.crt
        type: File
      name: file6
    - hostPath:
        path: /var/lib/rancher/rke2/server/tls/kube-scheduler
        type: Directory
      name: extra-mount-0
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 128Mi
      containerID: containerd://7e548c3f5e47a170fec41af1ba086c333a19724815816d7aef6dd6f6eaf8c3df
      image: docker.io/rancher/hardened-kubernetes:v1.34.2-rke2r1-build20251112
      imageID: sha256:7b491b625ab3a5905fb43f5778a0c0066493b638295ee423f02c55d17920e384
      lastState: {}
      name: kube-scheduler
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:52:25Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/lib/rancher/rke2/server/db/etcd/name
        name: file0
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig
        name: file1
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/client-scheduler.crt
        name: file2
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/client-scheduler.key
        name: file3
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.crt
        name: file4
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.key
        name: file5
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/server-ca.crt
        name: file6
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/rancher/rke2/server/tls/kube-scheduler
        name: extra-mount-0
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    phase: Running
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: Burstable
    startTime: "2025-12-30T21:51:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-30T21:46:54Z"
    generateName: rke2-canal-
    generation: 1
    labels:
      controller-revision-hash: 8445656b4c
      k8s-app: canal
      pod-template-generation: "1"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:controller-revision-hash: {}
            f:k8s-app: {}
            f:pod-template-generation: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"addfaf76-8730-4239-945a-f5e6fa237962"}: {}
        f:spec:
          f:affinity:
            .: {}
            f:nodeAffinity:
              .: {}
              f:requiredDuringSchedulingIgnoredDuringExecution: {}
          f:containers:
            k:{"name":"calico-node"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"CALICO_CNI_SERVICE_ACCOUNT"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"CALICO_DISABLE_FILE_LOGGING"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CALICO_NETWORKING_BACKEND"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CLUSTER_TYPE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"DATASTORE_TYPE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_DEFAULTENDPOINTTOHOSTACTION"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_FAILSAFEINBOUNDHOSTPORTS"}:
                  .: {}
                  f:name: {}
                k:{"name":"FELIX_FAILSAFEOUTBOUNDHOSTPORTS"}:
                  .: {}
                  f:name: {}
                k:{"name":"FELIX_FEATUREDETECTOVERRIDE"}:
                  .: {}
                  f:name: {}
                k:{"name":"FELIX_HEALTHENABLED"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_IPTABLESBACKEND"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_IPTABLESMARKMASK"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_IPTABLESREFRESHINTERVAL"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_IPV6SUPPORT"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_LOGSEVERITYSCREEN"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_NFTABLESMARKMASK"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_NFTABLESREFRESHINTERVAL"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_PROMETHEUSMETRICSENABLED"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FELIX_XDPENABLED"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"IP_AUTODETECTION_METHOD"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NODENAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"USE_POD_CIDR"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"WAIT_FOR_DATASTORE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:lifecycle:
                .: {}
                f:preStop:
                  .: {}
                  f:exec:
                    .: {}
                    f:command: {}
              f:livenessProbe:
                .: {}
                f:exec:
                  .: {}
                  f:command: {}
                f:failureThreshold: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:readinessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/host/etc/cni/net.d"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/lib/modules"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/run/xtables.lock"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/lib/calico"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/log/calico/cni"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/run/calico"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/run/nodeagent"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
            k:{"name":"kube-flannel"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"FLANNELD_IFACE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"FLANNELD_IFACE_REGEX"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"FLANNELD_IP_MASQ"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"POD_NAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"POD_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/kube-flannel/"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/run/xtables.lock"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:initContainers:
            .: {}
            k:{"name":"flexvol-driver"}:
              .: {}
              f:command: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/host/driver"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
            k:{"name":"install-cni"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"CALICO_CNI_SERVICE_ACCOUNT"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"CNI_CONF_NAME"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CNI_MTU"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"CNI_NETWORK_CONFIG"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"KUBERNETES_NODE_NAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"SLEEP"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/host/etc/cni/net.d"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/host/opt/cni/bin"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"cni-bin-dir"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"cni-log-dir"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"cni-net-dir"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"flannel-cfg"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"flexvol-driver-host"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"lib-modules"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"policysync"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"var-lib-calico"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"var-run-calico"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"xtables-lock"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:46:54Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              f:observedGeneration: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:initContainerStatuses: {}
          f:observedGeneration: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:52:05Z"
    name: rke2-canal-jnjvb
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: rke2-canal
      uid: addfaf76-8730-4239-945a-f5e6fa237962
    resourceVersion: "8298"
    uid: 1ea64530-56e8-4fce-9098-02de5e008e4b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - isim-dev
    containers:
    - command:
      - start_runit
      env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: USE_POD_CIDR
        value: "true"
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CALICO_CNI_SERVICE_ACCOUNT
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.serviceAccountName
      - name: CALICO_NETWORKING_BACKEND
        value: none
      - name: CLUSTER_TYPE
        value: k8s,canal
      - name: FELIX_IPTABLESREFRESHINTERVAL
        value: "60"
      - name: FELIX_NFTABLESREFRESHINTERVAL
        value: "60"
      - name: FELIX_IPTABLESBACKEND
        value: auto
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_LOGSEVERITYSCREEN
        value: info
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_PROMETHEUSMETRICSENABLED
        value: "true"
      - name: FELIX_XDPENABLED
        value: "false"
      - name: FELIX_FAILSAFEINBOUNDHOSTPORTS
      - name: FELIX_FAILSAFEOUTBOUNDHOSTPORTS
      - name: FELIX_IPTABLESMARKMASK
        value: "0xffff0000"
      - name: FELIX_NFTABLESMARKMASK
        value: "0xffff0000"
      - name: IP_AUTODETECTION_METHOD
        value: first-found
      - name: FELIX_FEATUREDETECTOVERRIDE
      image: rancher/hardened-calico:v3.30.3-build20251015
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /usr/bin/calico-node
            - -shutdown
      livenessProbe:
        exec:
          command:
          - /usr/bin/calico-node
          - -felix-live
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /readiness
          port: 9099
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-szx4l
        readOnly: true
    - command:
      - /opt/bin/flanneld
      - --ip-masq
      - --kube-subnet-mgr
      - --iptables-forward-rules=false
      - --ip-blackhole-route
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: FLANNELD_IFACE
        valueFrom:
          configMapKeyRef:
            key: canal_iface
            name: rke2-canal-config
      - name: FLANNELD_IFACE_REGEX
        valueFrom:
          configMapKeyRef:
            key: canal_iface_regex
            name: rke2-canal-config
      - name: FLANNELD_IP_MASQ
        valueFrom:
          configMapKeyRef:
            key: masquerade
            name: rke2-canal-config
      image: rancher/hardened-flannel:v0.27.4-build20251015
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-szx4l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /opt/cni/bin/install
      env:
      - name: CALICO_CNI_SERVICE_ACCOUNT
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.serviceAccountName
      - name: CNI_CONF_NAME
        value: 10-canal.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: rke2-canal-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CNI_MTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: rke2-canal-config
      - name: SLEEP
        value: "false"
      image: rancher/hardened-calico:v3.30.3-build20251015
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-szx4l
        readOnly: true
    - command:
      - /usr/local/bin/flexvol.sh
      - -s
      - /usr/local/bin/flexvol
      - -i
      - flexvoldriver
      image: rancher/hardened-calico:v3.30.3-build20251015
      imagePullPolicy: IfNotPresent
      name: flexvol-driver
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-szx4l
        readOnly: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2.000001e+09
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: canal
    serviceAccountName: canal
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: "null"
      name: lib-modules
    - hostPath:
        path: /var/run/calico
        type: "null"
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: "null"
      name: var-lib-calico
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - configMap:
        defaultMode: 420
        name: rke2-canal-config
      name: flannel-cfg
    - hostPath:
        path: /opt/cni/bin
        type: "null"
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: "null"
      name: cni-net-dir
    - hostPath:
        path: /var/log/calico/cni
        type: "null"
      name: cni-log-dir
    - hostPath:
        path: /var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - hostPath:
        path: /var/lib/kubelet/volumeplugins/nodeagent~uds
        type: DirectoryOrCreate
      name: flexvol-driver-host
    - name: kube-api-access-szx4l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:59Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:05Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:05Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:54Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 250m
      containerID: containerd://5b4b581c076adb9fc221c4e16222c64fe1b3c79ff4401740d7858167b87c2a75
      image: docker.io/rancher/hardened-calico:v3.30.3-build20251015
      imageID: sha256:02dad2654303334c7d9c75524a37524a6953e1e3289b9c80123bf7f1fd02cae8
      lastState: {}
      name: calico-node
      ready: true
      resources:
        requests:
          cpu: 250m
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:46:59Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-szx4l
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://b4ff2b57707cd1256c95d0cef616dc95bddfaa9471c91c23f00512add4815706
      image: docker.io/rancher/hardened-flannel:v0.27.4-build20251015
      imageID: sha256:cd9765145172eb194c3c80ce1cb3a4fc3bd9bdfa3ee0a5750f9e883d88856bd4
      lastState: {}
      name: kube-flannel
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:46:59Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-szx4l
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    initContainerStatuses:
    - containerID: containerd://00d2dfbca9ed75c34dfc111212020c3b1d518a6d5d48f0b7220aa68931c9349a
      image: docker.io/rancher/hardened-calico:v3.30.3-build20251015
      imageID: sha256:02dad2654303334c7d9c75524a37524a6953e1e3289b9c80123bf7f1fd02cae8
      lastState: {}
      name: install-cni
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://00d2dfbca9ed75c34dfc111212020c3b1d518a6d5d48f0b7220aa68931c9349a
          exitCode: 0
          finishedAt: "2025-12-30T21:46:58Z"
          reason: Completed
          startedAt: "2025-12-30T21:46:57Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-szx4l
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://78465d360f4aa3f34be9c3c4640a06d28b725235992684c3e6576706ac14ff22
      image: docker.io/rancher/hardened-calico:v3.30.3-build20251015
      imageID: sha256:02dad2654303334c7d9c75524a37524a6953e1e3289b9c80123bf7f1fd02cae8
      lastState: {}
      name: flexvol-driver
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://78465d360f4aa3f34be9c3c4640a06d28b725235992684c3e6576706ac14ff22
          exitCode: 0
          finishedAt: "2025-12-30T21:46:58Z"
          reason: Completed
          startedAt: "2025-12-30T21:46:58Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /host/driver
        name: flexvol-driver-host
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-szx4l
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: Burstable
    startTime: "2025-12-30T21:46:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 8ac4f070536272c42c565853d1259018193908fa40f3156f6ebcc02a374541f6
      cni.projectcalico.org/containerID: 8d309d273a6998cf72a595cf9a6b3b78515f4725eee4fa58af8e31e08adc8b33
      cni.projectcalico.org/podIP: 10.52.0.7/32
      cni.projectcalico.org/podIPs: 10.52.0.7/32
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.7"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2025-12-30T21:46:54Z"
    generateName: rke2-coredns-rke2-coredns-774b4f5cd4-
    generation: 1
    labels:
      app.kubernetes.io/instance: rke2-coredns
      app.kubernetes.io/name: rke2-coredns
      k8s-app: kube-dns
      pod-template-hash: 774b4f5cd4
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:checksum/config: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/name: {}
            f:k8s-app: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"497b34d9-b1d2-4600-b17b-040c6b069f1b"}: {}
        f:spec:
          f:affinity:
            .: {}
            f:podAntiAffinity:
              .: {}
              f:requiredDuringSchedulingIgnoredDuringExecution: {}
          f:containers:
            k:{"name":"coredns"}:
              .: {}
              f:args: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":53,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":53,"protocol":"UDP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":9153,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
              f:readinessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources:
                .: {}
                f:limits:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:add: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/coredns"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"config-volume"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:items: {}
                f:name: {}
              f:name: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:46:54Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
      manager: kube-scheduler
      operation: Update
      subresource: status
      time: "2025-12-30T21:46:54Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:13Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:13Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"10.52.0.7"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:51:59Z"
    name: rke2-coredns-rke2-coredns-774b4f5cd4-x7zhx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rke2-coredns-rke2-coredns-774b4f5cd4
      uid: 497b34d9-b1d2-4600-b17b-040c6b069f1b
    resourceVersion: "8172"
    uid: 89719ce8-d8ae-4de9-bb0f-3447342a532f
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - kube-dns
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/hardened-coredns:v1.13.1-build20251015
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: udp-53
        protocol: UDP
      - containerPort: 53
        name: tcp-53
        protocol: TCP
      - containerPort: 9153
        name: tcp-9153
        protocol: TCP
      readinessProbe:
        failureThreshold: 1
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4f8tr
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2e+09
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node-role.kubernetes.io/etcd
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: rke2-coredns-rke2-coredns
      name: config-volume
    - name: kube-api-access-4f8tr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:47:14Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:59Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:59Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 128Mi
      containerID: containerd://de27a4bffc3329fdcd99bae5b9e430c8df865842cf3f743f425588ac00b57c83
      image: docker.io/rancher/hardened-coredns:v1.13.1-build20251015
      imageID: sha256:f4ccc0f5d7de071b6b05988f9a7f10b14eda0db2351d56adc0b1a6b7dc356688
      lastState: {}
      name: coredns
      ready: true
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 100m
          memory: 128Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:47:13Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4f8tr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Running
    podIP: 10.52.0.7
    podIPs:
    - ip: 10.52.0.7
    qosClass: Guaranteed
    startTime: "2025-12-30T21:46:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: a581a6d7c915486093b9a458d5ad0bb68f82b62945fa815fd4fd38281ae7e695
      cni.projectcalico.org/containerID: c91a64601ea75f444402f4eccb27d02d626334ee88829d951641460974fc5a9a
      cni.projectcalico.org/podIP: 10.52.0.9/32
      cni.projectcalico.org/podIPs: 10.52.0.9/32
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.9"
            ],
            "default": true,
            "dns": {}
        }]
      scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
    creationTimestamp: "2025-12-30T21:46:54Z"
    generateName: rke2-coredns-rke2-coredns-autoscaler-5498b7ff8f-
    generation: 1
    labels:
      app.kubernetes.io/instance: rke2-coredns
      app.kubernetes.io/name: rke2-coredns-autoscaler
      k8s-app: kube-dns-autoscaler
      pod-template-hash: 5498b7ff8f
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:checksum/configmap: {}
            f:scheduler.alpha.kubernetes.io/tolerations: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/name: {}
            f:k8s-app: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"186219d9-143e-44a8-b08c-3a585c61833e"}: {}
        f:spec:
          f:containers:
            k:{"name":"autoscaler"}:
              .: {}
              f:command: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:resources:
                .: {}
                f:limits:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:46:54Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
      manager: kube-scheduler
      operation: Update
      subresource: status
      time: "2025-12-30T21:46:54Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:14Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:14Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"10.52.0.9"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:15Z"
    name: rke2-coredns-rke2-coredns-autoscaler-5498b7ff8f-5zllp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rke2-coredns-rke2-coredns-autoscaler-5498b7ff8f
      uid: 186219d9-143e-44a8-b08c-3a585c61833e
    resourceVersion: "1068"
    uid: e278706f-13f0-4d9f-9c91-76dbe8786e90
  spec:
    containers:
    - command:
      - /cluster-proportional-autoscaler
      - --namespace=kube-system
      - --configmap=rke2-coredns-rke2-coredns-autoscaler
      - --target=Deployment/rke2-coredns-rke2-coredns
      - --logtostderr=true
      - --v=2
      image: rancher/hardened-cluster-autoscaler:v1.10.2-build20251015
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: autoscaler
      resources:
        limits:
          cpu: 100m
          memory: 64Mi
        requests:
          cpu: 25m
          memory: 16Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-82j8n
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2e+09
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rke2-coredns-rke2-coredns-autoscaler
    serviceAccountName: rke2-coredns-rke2-coredns-autoscaler
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node-role.kubernetes.io/etcd
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-82j8n
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:47:15Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:47:15Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:47:15Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 25m
        memory: 16Mi
      containerID: containerd://ce558c1e58e278584708de57529b2852989d9179d17903b136015242e06fe950
      image: docker.io/rancher/hardened-cluster-autoscaler:v1.10.2-build20251015
      imageID: sha256:88a47d41f8b82eb1c0afce55f7e4cd07ce7ea2d8f254330d46d4280d5200fd4f
      lastState: {}
      name: autoscaler
      ready: true
      resources:
        limits:
          cpu: 100m
          memory: 64Mi
        requests:
          cpu: 25m
          memory: 16Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:47:14Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-82j8n
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Running
    podIP: 10.52.0.9
    podIPs:
    - ip: 10.52.0.9
    qosClass: Burstable
    startTime: "2025-12-30T21:46:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 4b9af66d7d3603455c63cbfe3413df5440b49a32806a2f8b503e5d96fadaebf7
      cni.projectcalico.org/podIP: 10.52.0.82/32
      cni.projectcalico.org/podIPs: 10.52.0.82/32
      harvesterhci.io/timestamp: "2025-12-30T21:50:35Z"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.82"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2025-12-30T21:50:48Z"
    generateName: rke2-ingress-nginx-controller-
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: rke2-ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: rke2-ingress-nginx
      app.kubernetes.io/part-of: rke2-ingress-nginx
      app.kubernetes.io/version: 1.13.4
      controller-revision-hash: 66bb6c75b4
      helm.sh/chart: rke2-ingress-nginx-4.13.400
      pod-template-generation: "3"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:50:48Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:harvesterhci.io/timestamp: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:app.kubernetes.io/version: {}
            f:controller-revision-hash: {}
            f:helm.sh/chart: {}
            f:pod-template-generation: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"d81b7835-408a-487f-8a99-5217352a50aa"}: {}
        f:spec:
          f:affinity:
            .: {}
            f:nodeAffinity:
              .: {}
              f:requiredDuringSchedulingIgnoredDuringExecution: {}
          f:automountServiceAccountToken: {}
          f:containers:
            k:{"name":"rke2-ingress-nginx-controller"}:
              .: {}
              f:args: {}
              f:env:
                .: {}
                k:{"name":"LD_PRELOAD"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"POD_NAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"POD_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:lifecycle:
                .: {}
                f:preStop:
                  .: {}
                  f:exec:
                    .: {}
                    f:command: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":80,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":443,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":8444,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
              f:readinessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:add: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
                f:runAsGroup: {}
                f:runAsNonRoot: {}
                f:runAsUser: {}
                f:seccompProfile:
                  .: {}
                  f:type: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/usr/local/certificates/"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"webhook-cert"}:
              .: {}
              f:name: {}
              f:secret:
                .: {}
                f:defaultMode: {}
                f:secretName: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:50:48Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:50:48Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"10.52.0.82"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:52:04Z"
    name: rke2-ingress-nginx-controller-wqw8b
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: rke2-ingress-nginx-controller
      uid: d81b7835-408a-487f-8a99-5217352a50aa
    resourceVersion: "8265"
    uid: 4056f502-547e-4f4a-9437-687072e28334
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - isim-dev
    automountServiceAccountToken: true
    containers:
    - args:
      - /nginx-ingress-controller
      - --election-id=rke2-ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/rke2-ingress-nginx-controller
      - --validating-webhook=:8444
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      - --watch-ingress-without-class=true
      - --default-ssl-certificate=cattle-system/tls-rancher-internal
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: rancher/nginx-ingress-controller:v1.13.4-hardened1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: rke2-ingress-nginx-controller
      ports:
      - containerPort: 80
        hostPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        hostPort: 443
        name: https
        protocol: TCP
      - containerPort: 8444
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 90Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 82
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-drdpd
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rke2-ingress-nginx
    serviceAccountName: rke2-ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: rke2-ingress-nginx-admission
    - name: kube-api-access-drdpd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:50:49Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:50:48Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:04Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:52:04Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:50:48Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 90Mi
      containerID: containerd://b435cd9b4d56e08e67709627e48917f542760150576a666a1b56e012eba7b192
      image: docker.io/rancher/nginx-ingress-controller:v1.13.4-hardened1
      imageID: sha256:f184bd6520d3a2ac21c05eea790dbc6876e235d87fc738d12e89e33ed1e70f23
      lastState: {}
      name: rke2-ingress-nginx-controller
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 90Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:50:48Z"
      user:
        linux:
          gid: 82
          supplementalGroups:
          - 82
          uid: 101
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-drdpd
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Running
    podIP: 10.52.0.82
    podIPs:
    - ip: 10.52.0.82
    qosClass: Burstable
    startTime: "2025-12-30T21:50:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: c39151d65318180fe927ff9973a2e9c4248ef26fa7aa1614044b21c419eac468
      cni.projectcalico.org/podIP: 10.52.0.5/32
      cni.projectcalico.org/podIPs: 10.52.0.5/32
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.5"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2025-12-30T21:47:11Z"
    generateName: rke2-metrics-server-7c7bf879f6-
    generation: 1
    labels:
      app: rke2-metrics-server
      app.kubernetes.io/instance: rke2-metrics-server
      app.kubernetes.io/name: rke2-metrics-server
      pod-template-hash: 7c7bf879f6
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:app: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/name: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"03ab58d6-6d25-41c0-9cf1-d195889e9961"}: {}
        f:spec:
          f:containers:
            k:{"name":"metrics-server"}:
              .: {}
              f:args: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":10250,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
              f:readinessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
                f:runAsNonRoot: {}
                f:runAsUser: {}
                f:seccompProfile:
                  .: {}
                  f:type: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/tmp"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:volumes:
            .: {}
            k:{"name":"tmp"}:
              .: {}
              f:emptyDir: {}
              f:name: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:47:11Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:12Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:47:12Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"10.52.0.5"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:51:58Z"
    name: rke2-metrics-server-7c7bf879f6-t4fj7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rke2-metrics-server-7c7bf879f6
      uid: 03ab58d6-6d25-41c0-9cf1-d195889e9961
    resourceVersion: "8102"
    uid: cb426a28-1fc6-408f-bb3c-64e1244efdc3
  spec:
    containers:
    - args:
      - --secure-port=10250
      - --cert-dir=/tmp
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      image: rancher/hardened-k8s-metrics-server:v0.8.0-build20251015
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9dm8h
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2e+09
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rke2-metrics-server
    serviceAccountName: rke2-metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-9dm8h
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:47:13Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:47:11Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:58Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:51:58Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:47:11Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 200Mi
      containerID: containerd://ecaa2525d4ea89a9ea7d92aa6ee990a6dd9a16732f0ff32e7b934fa526032224
      image: docker.io/rancher/hardened-k8s-metrics-server:v0.8.0-build20251015
      imageID: sha256:be573c48df1165efecd3d58b46c543e69c636bcea63abc90445c9fe043676f12
      lastState: {}
      name: metrics-server
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:47:12Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 1000
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9dm8h
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Running
    podIP: 10.52.0.5
    podIPs:
    - ip: 10.52.0.5
    qosClass: Burstable
    startTime: "2025-12-30T21:47:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: b53fcdb801cb9610f0b5f64bedc7ab9a8124dc42e0ff49982453580bc1f50cb1
    creationTimestamp: "2025-12-30T21:46:53Z"
    generateName: rke2-multus-
    generation: 1
    labels:
      app: rke2-multus
      controller-revision-hash: 7bd77d985b
      pod-template-generation: "1"
      tier: node
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:checksum/config: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:app: {}
            f:controller-revision-hash: {}
            f:pod-template-generation: {}
            f:tier: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e3a908e3-3929-4a06-9ee6-42c95a952a56"}: {}
        f:spec:
          f:affinity:
            .: {}
            f:nodeAffinity:
              .: {}
              f:requiredDuringSchedulingIgnoredDuringExecution: {}
          f:containers:
            k:{"name":"kube-rke2-multus"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"KUBERNETES_NODE_NAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/host/etc/cni/net.d"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/host/opt/cni/bin"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/tmp/multus-conf/00-multus.conf"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:subPath: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:initContainers:
            .: {}
            k:{"name":"cni-plugins"}:
              .: {}
              f:env:
                .: {}
                k:{"name":"SKIP_CNI_BINARIES"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/host/opt/cni/bin"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"cni"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"cnibin"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"multus-cfg"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:46:53Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              f:observedGeneration: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:observedGeneration: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:initContainerStatuses: {}
          f:observedGeneration: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.122.168"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:46:59Z"
    name: rke2-multus-fhvxv
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: rke2-multus
      uid: e3a908e3-3929-4a06-9ee6-42c95a952a56
    resourceVersion: "828"
    uid: 772c3f90-6f35-4db5-8c95-eee5873b8b76
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - isim-dev
    containers:
    - args:
      - --multus-conf-file=auto
      - --cni-conf-dir=/host/etc/cni/net.d
      - --cni-bin-dir=/host/opt/cni/bin
      - --multus-autoconfig-dir=/host/etc/cni/net.d
      - --multus-kubeconfig-file-host=/etc/cni/net.d/multus.d/multus.kubeconfig
      command:
      - /thin_entrypoint
      env:
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: rancher/hardened-multus-cni:v4.2.3-build20251031
      imagePullPolicy: IfNotPresent
      name: kube-rke2-multus
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/etc/cni/net.d
        name: cni
      - mountPath: /host/opt/cni/bin
        name: cnibin
      - mountPath: /tmp/multus-conf/00-multus.conf
        name: multus-cfg
        subPath: cni-conf.json
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vlhtz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - env:
      - name: SKIP_CNI_BINARIES
        value: flannel
      image: rancher/hardened-cni-plugins:v1.8.0-build20251014
      imagePullPolicy: IfNotPresent
      name: cni-plugins
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cnibin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vlhtz
        readOnly: true
    nodeName: isim-dev
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2.000001e+09
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: multus
    serviceAccountName: multus
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/cni/net.d
        type: "null"
      name: cni
    - hostPath:
        path: /opt/cni/bin
        type: "null"
      name: cnibin
    - configMap:
        defaultMode: 420
        name: rke2-multus-v4.2.300-config
      name: multus-cfg
    - name: kube-api-access-vlhtz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:55Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:56Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:59Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:59Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:46:53Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a53dda94fe11591a97825885d8d69fa7f87d4173525b6ba70293dcfc3127774d
      image: docker.io/rancher/hardened-multus-cni:v4.2.3-build20251031
      imageID: sha256:9b5fedfc193d00cb1e67e8ce1950c1ec7f79a0e45477758a09a72fcf378915ed
      lastState:
        terminated:
          containerID: containerd://c2db0c9b090a9f4ee3e8356b5add01f0fc7cad9d4d8d6f0806218aa994ed4e34
          exitCode: 0
          finishedAt: "2025-12-30T21:46:57Z"
          reason: Completed
          startedAt: "2025-12-30T21:46:57Z"
      name: kube-rke2-multus
      ready: true
      resources: {}
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:46:58Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /host/etc/cni/net.d
        name: cni
      - mountPath: /host/opt/cni/bin
        name: cnibin
      - mountPath: /tmp/multus-conf/00-multus.conf
        name: multus-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vlhtz
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    initContainerStatuses:
    - containerID: containerd://9df474570168083ce3a532cc38c6a011ef3215750c167283e90752d52299c8eb
      image: docker.io/rancher/hardened-cni-plugins:v1.8.0-build20251014
      imageID: sha256:7f9a05623fb5b93c1df1c76530524b37becc2e050ed003747cad7c26e7e4a64d
      lastState: {}
      name: cni-plugins
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://9df474570168083ce3a532cc38c6a011ef3215750c167283e90752d52299c8eb
          exitCode: 0
          finishedAt: "2025-12-30T21:46:55Z"
          reason: Completed
          startedAt: "2025-12-30T21:46:55Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cnibin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vlhtz
        readOnly: true
        recursiveReadOnly: Disabled
    observedGeneration: 1
    phase: Running
    podIP: 192.168.122.168
    podIPs:
    - ip: 192.168.122.168
    qosClass: BestEffort
    startTime: "2025-12-30T21:46:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: dbafb4fef3341b511214ee35a6b74a9a24fe3705f29636359fa4c6db80ae2237
      cni.projectcalico.org/podIP: 10.52.0.38/32
      cni.projectcalico.org/podIPs: 10.52.0.38/32
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.38"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2025-12-30T21:49:31Z"
    generateName: snapshot-controller-8b5bf4d55-
    generation: 1
    labels:
      app: snapshot-controller
      pod-template-hash: 8b5bf4d55
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:app: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"da6b05bf-99db-4a54-a63b-98f94fe77db4"}: {}
        f:spec:
          f:containers:
            k:{"name":"snapshot-controller"}:
              .: {}
              f:args: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:49:31Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:32Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:32Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"10.52.0.38"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:38Z"
    name: snapshot-controller-8b5bf4d55-8swl5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: snapshot-controller-8b5bf4d55
      uid: da6b05bf-99db-4a54-a63b-98f94fe77db4
    resourceVersion: "4421"
    uid: 2a54ec73-9b88-4e43-8ab9-00bd42996b55
  spec:
    containers:
    - args:
      - --v=5
      - --leader-election=true
      image: registry.k8s.io/sig-storage/snapshot-controller:v8.3.0
      imagePullPolicy: IfNotPresent
      name: snapshot-controller
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xmwjx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: snapshot-controller
    serviceAccountName: snapshot-controller
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-xmwjx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:38Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:31Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:38Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:38Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://17ae819d2f162617a419e4f7f44357f59d349cbaa719d9d81fe4a2eafe8c78b6
      image: registry.k8s.io/sig-storage/snapshot-controller:v8.3.0
      imageID: sha256:82e5ea8a09d07f037d1292be7e2fb502939182084a1e9c28e779c8cf92f2948a
      lastState: {}
      name: snapshot-controller
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:49:37Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xmwjx
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Running
    podIP: 10.52.0.38
    podIPs:
    - ip: 10.52.0.38
    qosClass: BestEffort
    startTime: "2025-12-30T21:49:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 29ce09af0a4bc0c77542dc72f7c2a9561f8c28055add7a7edbf017c09b93cbd3
      cni.projectcalico.org/podIP: 10.52.0.42/32
      cni.projectcalico.org/podIPs: 10.52.0.42/32
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.42"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2025-12-30T21:49:31Z"
    generateName: snapshot-controller-8b5bf4d55-
    generation: 1
    labels:
      app: snapshot-controller
      pod-template-hash: 8b5bf4d55
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:app: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"da6b05bf-99db-4a54-a63b-98f94fe77db4"}: {}
        f:spec:
          f:containers:
            k:{"name":"snapshot-controller"}:
              .: {}
              f:args: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:49:31Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:32Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:32Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"10.52.0.42"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:38Z"
    name: snapshot-controller-8b5bf4d55-hmjk2
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: snapshot-controller-8b5bf4d55
      uid: da6b05bf-99db-4a54-a63b-98f94fe77db4
    resourceVersion: "4375"
    uid: 37788262-5513-4879-994f-bec185cad171
  spec:
    containers:
    - args:
      - --v=5
      - --leader-election=true
      image: registry.k8s.io/sig-storage/snapshot-controller:v8.3.0
      imagePullPolicy: IfNotPresent
      name: snapshot-controller
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzlkt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: snapshot-controller
    serviceAccountName: snapshot-controller
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-vzlkt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:37Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:31Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:37Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://257178d01cefd7d1824e9093f346d62fae9271072602249c200bcc96693b566e
      image: registry.k8s.io/sig-storage/snapshot-controller:v8.3.0
      imageID: sha256:82e5ea8a09d07f037d1292be7e2fb502939182084a1e9c28e779c8cf92f2948a
      lastState: {}
      name: snapshot-controller
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:49:37Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzlkt
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Running
    podIP: 10.52.0.42
    podIPs:
    - ip: 10.52.0.42
    qosClass: BestEffort
    startTime: "2025-12-30T21:49:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: b73b92603cdd1e7a7fb3648a73b6e7ed1501c74988412f6981f2c3adff8512f2
      cni.projectcalico.org/podIP: 10.52.0.47/32
      cni.projectcalico.org/podIPs: 10.52.0.47/32
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "k8s-pod-network",
            "ips": [
                "10.52.0.47"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2025-12-30T21:49:31Z"
    generateName: whereabouts-controller-5f4dff78f6-
    generation: 1
    labels:
      app: whereabouts-controller
      pod-template-hash: 5f4dff78f6
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:app: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"c562691c-950f-4df2-be0f-2447ec8f33a5"}: {}
        f:spec:
          f:containers:
            k:{"name":"whereabouts"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"NODENAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"WHEREABOUTS_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources:
                .: {}
                f:limits:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/cron-schedule"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/host/etc/cni/net.d"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/host/opt/cni/bin"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:preemptionPolicy: {}
          f:priority: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:volumes:
            .: {}
            k:{"name":"cni-net-dir"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"cnibin"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"cron-scheduler-configmap"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:items: {}
                f:name: {}
              f:name: {}
            k:{"name":"kube-api-access-6kd6k"}:
              .: {}
              f:name: {}
              f:projected:
                .: {}
                f:defaultMode: {}
                f:sources: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-12-30T21:49:31Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:cni.projectcalico.org/containerID: {}
            f:cni.projectcalico.org/podIP: {}
            f:cni.projectcalico.org/podIPs: {}
      manager: calico
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:32Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:k8s.v1.cni.cncf.io/network-status: {}
      manager: multus
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:32Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"10.52.0.47"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2025-12-30T21:49:37Z"
    name: whereabouts-controller-5f4dff78f6-gntv2
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: whereabouts-controller-5f4dff78f6
      uid: c562691c-950f-4df2-be0f-2447ec8f33a5
    resourceVersion: "4360"
    uid: 8b11a906-3abd-4ea7-9434-da0dc2b3c44a
  spec:
    containers:
    - command:
      - /node-slice-controller
      env:
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: WHEREABOUTS_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: ghcr.io/k8snetworkplumbingwg/whereabouts:v0.9.2
      imagePullPolicy: IfNotPresent
      name: whereabouts
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cnibin
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /cron-schedule
        name: cron-scheduler-configmap
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6kd6k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: isim-dev
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: harvester-whereabouts
    serviceAccountName: harvester-whereabouts
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /opt/cni/bin
        type: "null"
      name: cnibin
    - hostPath:
        path: /etc/cni/net.d
        type: "null"
      name: cni-net-dir
    - configMap:
        defaultMode: 484
        items:
        - key: cron-expression
          path: config
        name: whereabouts-config
      name: cron-scheduler-configmap
    - name: kube-api-access-6kd6k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:37Z"
      observedGeneration: 1
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:31Z"
      observedGeneration: 1
      status: "True"
      type: Initialized
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:37Z"
      observedGeneration: 1
      status: "True"
      type: ContainersReady
    - lastProbeTime: "null"
      lastTransitionTime: "2025-12-30T21:49:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 100Mi
      containerID: containerd://c45563a256ca39034f4ad04e6f493a5a34f65276acf22b58b235996da703fcf8
      image: ghcr.io/k8snetworkplumbingwg/whereabouts:v0.9.2
      imageID: sha256:fb5d855c1e441aff51cdd8c2929d8051612b1023d9bfe66ca25658efba2332cd
      lastState: {}
      name: whereabouts
      ready: true
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 100Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-30T21:49:36Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          - 1
          - 2
          - 3
          - 4
          - 6
          - 10
          - 11
          - 20
          - 26
          - 27
          uid: 0
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cnibin
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /cron-schedule
        name: cron-scheduler-configmap
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6kd6k
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.122.168
    hostIPs:
    - ip: 192.168.122.168
    observedGeneration: 1
    phase: Running
    podIP: 10.52.0.47
    podIPs:
    - ip: 10.52.0.47
    qosClass: Burstable
    startTime: "2025-12-30T21:49:31Z"
kind: List
metadata:
  resourceVersion: "14473"
